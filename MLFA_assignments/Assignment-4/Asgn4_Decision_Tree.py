# -*- coding: utf-8 -*-
"""asgn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PfSYpWNc5cBQKug9e-34FOXOAaV-2-kY
"""

#Gohil Happy
#21IM30006
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

"""# Getting the data
1. Price Buying: buying price (v-high, high, med, low)
2. Price Maintenance: price of the maintenance (v-high, high, med, low)
3. Doors: Number of doors (2, 3, 4, 5-more)
4. Persons: Capacity in terms of persons to carry (2, 4, more)
5. Lug_boot: The size of luggage boot (small, med, big)
6. Safety: Safety rating of the car (low, med, high)
7. Acceptability
"""

df = pd.read_csv('car_evaluation.csv', names=['price_B','price_M','doors','persons','lug_boot','safety','acc'])

"""# Visulizing the data"""

df.head()

df.acc.unique()

label_encoder = LabelEncoder()
df['price_B'] = label_encoder.fit_transform(df['price_B'])
df['price_M'] = label_encoder.fit_transform(df['price_M'])
df['doors'] = label_encoder.fit_transform(df['doors'])
df['persons'] = label_encoder.fit_transform(df['persons'])
df['lug_boot'] = label_encoder.fit_transform(df['lug_boot'])
df['safety'] = label_encoder.fit_transform(df['safety'])
df['acc'] = label_encoder.fit_transform(df['acc'])

df.head()

df.info()

"""# missing values"""

df.isnull().sum()

"""# Preprocessing dataset"""

X = df.iloc[:,:-1]
y = df.iloc[:,-1]
X = X.to_numpy()
y = y.to_numpy()

class TreeNode:
    def __init__(self, data, feature=None):
        self.data = data
        self.feature = feature
        self.children = {}

    def predict_tree(self, X):
        if self.feature is None:
            return self.data
        if X[self.feature] in self.children:
            return self.children[X[self.feature]].predict_tree(X)
        else:
            return self.data

    def count_nodes(self):
        # Initialize count with 1 (for the current node)
        count = 1
        for child in self.children.values():
            count += child.count_nodes()
        return count
""" Calculating the entropy"""
def entropy(y):
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-6))

""" to calculate information gain for each branch formation"""
def information_gain(X, y, feature):
    parent_entropy = entropy(y)
    unique_values = np.unique(X[:, feature])
    weighted_child_entropy = 0

    for value in unique_values:
        child_indices = np.where(X[:, feature] == value)
        child_entropy = entropy(y[child_indices])
        weight = len(child_indices[0]) / len(y)
        weighted_child_entropy += weight * child_entropy

    return parent_entropy - weighted_child_entropy

""" calculate the accuracy of tree"""
def evaluate_accuracy(tree, X_test, y_test):
    correct_predictions = 0
    total_samples = len(y_test)

    for i in range(total_samples):
        prediction = tree.predict_tree(X_test[i])
        if prediction == y_test[i]:
            correct_predictions += 1

    accuracy = correct_predictions / total_samples
    return accuracy

"""" Spliting the data into 60:20:20 """
from sklearn.model_selection import train_test_split
X_train, X_t, y_train, y_t = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

"""Definign the recursive build_tree method"""
def build_tree(X, y, X_val, y_val, features, entropy_threshold=0.0, validation_accuracies=None, train_accuracies=None):
    current_entropy = entropy(y)

    if len(np.unique(y)) == 1 or current_entropy <= entropy_threshold:
        return TreeNode(data=y[0])

    if len(features) == 0:
        return TreeNode(data=np.bincount(y).argmax())

    best_feature = max(features, key=lambda feature: information_gain(X, y, feature))
    node = TreeNode(data=None, feature=best_feature)

    unique_values = np.unique(X[:, best_feature])
    for value in unique_values:
        child_indices = np.where(X[:, best_feature] == value)
        child_x = X[child_indices]
        child_y = y[child_indices]
        child_features = [f for f in features if f != best_feature]
        child_node = build_tree(child_x, child_y, X_val, y_val, child_features, entropy_threshold, validation_accuracies, train_accuracies)
        node.children[value] = child_node

    # Calculate validation accuracy after constructing all child nodes
    if validation_accuracies is not None:
        validation_prediction = [node.predict_tree(sample) for sample in X_val]
        validation_accuracy = np.mean(validation_prediction == y_val)
        validation_accuracies.append(validation_accuracy)

    if train_accuracies is not None:
        train_prediction = [node.predict_tree(sample) for sample in X]
        train_accuracy = np.mean(train_prediction == y)
        train_accuracies.append(train_accuracy)


    return node


# Initialize lists to store results
thresholds = [0, 0.25, 0.5, 0.75, 1]
train_accuracies = []
val_accuracies = []
tree_sizes = []

# Define a function to build and evaluate a decision tree with a given entropy threshold
def evaluate_tree(X_train, y_train, X_val, y_val, entropy_threshold):
    # Build the decision tree with the specified entropy threshold
    features = list(range(X_train.shape[1]))
    validation_accuracies = []
    test_accuracies = []
    root = build_tree(X_train, y_train, X_val, y_val, features, entropy_threshold, validation_accuracies, test_accuracies)

    # Calculate training accuracy
    training_prediction = [root.predict_tree(sample) for sample in X_train]
    training_accuracy = np.mean(training_prediction == y_train)

    # Calculate validation accuracy
    validation_accuracy = validation_accuracies[-1]

    # Return training and validation accuracy along with the tree size
    return training_accuracy, validation_accuracy, root.count_nodes()

# Iterate over different entropy thresholds
for threshold in thresholds:
    train_acc, val_acc, tree_size = evaluate_tree(X_train, y_train, X_val, y_val, threshold)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    tree_sizes.append(tree_size)

# Create plots
plt.figure(figsize=(12, 5))

# Plot Percentage Accuracy vs. Threshold
plt.subplot(1, 2, 1)
plt.plot(thresholds, train_accuracies, marker='o', label='Training Accuracy')
plt.plot(thresholds, val_accuracies, marker='o', label='Validation Accuracy')
plt.xlabel('Entropy Threshold')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Entropy Threshold')
plt.legend()

# Plot Size of Decision Tree vs. Threshold
plt.subplot(1, 2, 2)
plt.plot(thresholds, tree_sizes, marker='o')
plt.xlabel('Entropy Threshold')
plt.ylabel('Tree Size')
plt.title('Decision Tree Size vs. Entropy Threshold')

# Show plots
plt.tight_layout()
plt.show()

thresholds_s = ['0', '0.25', '0.5', '0.75', '1']

plt.figure(figsize=(12, 5))
# Plot Percentage Accuracy vs. Threshold
plt.subplot(1, 3, 1)
plt.bar(thresholds_s, train_accuracies, label='Training Accuracy',color='orange')
plt.xlabel('Entropy Threshold')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Entropy Threshold')
plt.legend()

plt.subplot(1, 3, 2)
plt.bar(thresholds_s, val_accuracies, label='Validation Accuracy',)
plt.xlabel('Entropy Threshold')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Entropy Threshold')
plt.legend()

# Plot Size of Decision Tree vs. Threshold
plt.subplot(1, 3, 3)
plt.plot(thresholds_s, tree_sizes, marker='o')
plt.xlabel('Entropy Threshold')
plt.ylabel('Tree Size')
plt.title('Decision Tree Size vs. Entropy Threshold')

# Show plots
plt.tight_layout()
plt.show()

"""# Experiment 2(a)"""

# Initialize variables to store the best threshold and the corresponding test accuracy
best_threshold = None
best_accuracy = 0.0
test_a_list = []
# Iterate over each threshold value
for threshold in thresholds:
    # Build and evaluate the decision tree with the current threshold on the test data
    train_acc, test_acc, _ = evaluate_tree(X_train, y_train, X_test, y_test, threshold)
    test_a_list.append(test_acc)

    # Check if the current threshold gives a higher test accuracy
    if test_acc > best_accuracy:
        best_accuracy = test_acc
        best_threshold = threshold
        best_train_acc = train_acc

print("Optimal Entropy Threshold:", best_threshold)
print("Test Accuracy with Optimal Threshold:", best_accuracy)
print("Train Accuracy with optimal Threshold:", best_train_acc)

"""# Experiment 2(b)"""

validation_accuracies = []
train_accuracies = []
features = list(range(X_train.shape[1]))
entropy_threshold = 0.75
root = build_tree(X_train, y_train, X_val, y_val, features, entropy_threshold, validation_accuracies, train_accuracies)
print(len(validation_accuracies))
print(len(train_accuracies))
plt.plot(np.arange(len(train_accuracies)), train_accuracies, marker='o', label='Training Accuracy')
plt.plot(np.arange(len(validation_accuracies)), validation_accuracies, marker='o', label='Validation Accuracy')
plt.xlabel('Entropy Threshold')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Entropy Threshold')
plt.legend()
print(root.count_nodes())

"""# Experiment 2 (c)"""

class TreeNode:
    def __init__(self, data, feature=None):
        self.data = data
        self.feature = feature
        self.children = {}

    def predict_tree(self, X):
        if self.feature is None:
            return self.data
        if X[self.feature] in self.children:
            return self.children[X[self.feature]].predict_tree(X)
        else:
            return self.data

def entropy(y):
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-6))

def information_gain(X, y, feature):
    parent_entropy = entropy(y)
    unique_values = np.unique(X[:, feature])
    weighted_child_entropy = 0

    for value in unique_values:
        child_indices = np.where(X[:, feature] == value)
        child_entropy = entropy(y[child_indices])
        weight = len(child_indices[0]) / len(y)
        weighted_child_entropy += weight * child_entropy

    return parent_entropy - weighted_child_entropy

def build_tree(X_train, y_train, X_val, y_val, features, max_depth=None, min_samples_split=2, consecutive_no_improvement_limit=3):
    current_entropy = entropy(y_train)

    if len(np.unique(y_train)) == 1 or current_entropy == 0:
        return TreeNode(data=y_train[0])

    if len(features) == 0 or (max_depth is not None and max_depth <= 0) or len(X_train) < min_samples_split:
        return TreeNode(data=np.bincount(y_train).argmax())

    best_feature = max(features, key=lambda feature: information_gain(X_train, y_train, feature))
    node = TreeNode(data=None, feature=best_feature)

    unique_values = np.unique(X_train[:, best_feature])
    consecutive_no_improvement = 0

    # Initialize best_val_accuracy here
    best_val_accuracy = 0

    for value in unique_values:
        child_indices = np.where(X_train[:, best_feature] == value)
        child_x = X_train[child_indices]
        child_y = y_train[child_indices]
        child_features = [f for f in features if f != best_feature]

        child_node = build_tree(child_x, child_y, X_val, y_val, child_features, max_depth=max_depth - 1 if max_depth is not None else None, min_samples_split=min_samples_split, consecutive_no_improvement_limit=consecutive_no_improvement_limit)
        node.children[value] = child_node

        # Calculate accuracy on the validation dataset
        val_accuracy = evaluate_accuracy(child_node, X_val, y_val)

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            consecutive_no_improvement = 0
        else:
            consecutive_no_improvement += 1

        # If validation accuracy doesn't improve for a certain number of consecutive iterations, stop growing the tree
        if consecutive_no_improvement >= consecutive_no_improvement_limit:
            break

    return node

def evaluate_accuracy(tree, X_test, y_test):
    correct_predictions = 0
    total_samples = len(y_test)

    for i in range(total_samples):
        prediction = tree.predict_tree(X_test[i])
        if prediction == y_test[i]:
            correct_predictions += 1

    accuracy = correct_predictions / total_samples
    return accuracy

# Split the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Initialize variables to track the best model
best_val_accuracy = 0
best_tree = None

# Initialize a counter for consecutive iterations without accuracy improvement
consecutive_no_improvement = 0

# Maximum number of consecutive iterations without improvement allowed
max_consecutive_no_improvement = 3

# Create and train the decision tree with early stopping
while consecutive_no_improvement <= max_consecutive_no_improvement:
    tree = build_tree(X_train, y_train, X_val, y_val, list(range(X_train.shape[1])))

    # Calculate accuracy on the validation dataset
    val_accuracy = evaluate_accuracy(tree, X_val, y_val)

    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_tree = tree
        consecutive_no_improvement = 0
    else:
        consecutive_no_improvement += 1

# Evaluate the best tree on the testing dataset
test_accuracy = evaluate_accuracy(best_tree, X_test, y_test)

# Print the results
print("Best Validation Accuracy:", best_val_accuracy)
print("Testing Accuracy with Early Stopping:", test_accuracy)

"""# Experiment 3"""

# Assuming y contains your target variable
unique_class_labels = np.unique(y)

# Update the class_labels list with unique class labels
class_labels = list(unique_class_labels)

def print_rules(node, feature_names, class_labels, antecedent="IF"):
    if node is None:
        return

    if node.feature is not None:
        if antecedent != "IF ":
            antecedent += " AND "

        feature_name = feature_names[node.feature]
        unique_values = list(node.children.keys())

        for value in unique_values:
            print_rules(node.children[value], feature_names, class_labels, f"{antecedent}{feature_name}={value}")
    else:
        # Leaf node, print the class label as the consequent
        consequent = f"THEN {class_labels[node.data]}"
        print(f"{antecedent} => {consequent}")

# Call this function to print rules for Experiment 1 or 2 decision tree (e.g., root)
print_rules(root, feature_names=["price_B", "price_M", "doors", "persons", "lug_boot", "safety"], class_labels=class_labels)

import sys

# Create and open a text file for writing the rules
with open('decision_tree_rules.txt', 'w') as file:
    # Redirect the standard output to the text file
    sys.stdout = file

    # Call the function to print rules (replace 'tree', feature_names, and class_labels with your values)
    print_rules(tree, feature_names=["price_B", "price_M", "doors", "persons", "lug_boot", "safety"], class_labels=class_labels)

# Reset the standard output to its default
sys.stdout = sys.__stdout__

print("Decision tree rules have been written to 'decision_tree_rules.txt'")

